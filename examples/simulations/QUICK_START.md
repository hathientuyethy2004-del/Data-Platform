# Quick Start & Configuration Guide

## ğŸš€ Start in 3 Steps

### Step 1: Navigate to Simulations Directory
```bash
cd simulations/
```

### Step 2: Start All Services
```bash
docker-compose -f docker-compose-production.yml up -d
```

### Step 3: Verify Everything is Running
```bash
# Check all containers
docker-compose ps

# Expected output:
# NAME                      STATUS
# zookeeper                 Up
# kafka                     Up (healthy)
# schema-registry          Up (healthy)
# kafka-ui                 Up
# api-gateway              Up
# mobile-simulator         Up
# web-simulator            Up
# postgres                 Up
# cdc-simulator            Up
# spark-master             Up
# spark-worker             Up
# streaming-jobs          Up
# airflow-postgres        Up
# airflow-webserver       Up
# airflow-scheduler       Up
# clickstream-simulator   Up
# external-data-simulator Up
```

---

## ğŸ“Š Monitor Data in Real-Time

### Option 1: Kafka UI (Recommended)
Open browser â†’ `http://localhost:8080`
- Visual topic browser
- Consumer group monitoring
- Live message inspection
- Schema Registry integration

### Option 2: Command Line
```bash
# Watch app events (application data layer)
docker exec kafka kafka-console-consumer \
  --bootstrap-server kafka:9092 \
  --topic topic_app_events \
  --from-beginning \
  --max-messages 20

# Watch CDC events (database changes)
docker exec kafka kafka-console-consumer \
  --bootstrap-server kafka:9092 \
  --topic topic_cdc_changes \
  --from-beginning \
  --max-messages 20

# Watch clickstream (user interactions)
docker exec kafka kafka-console-consumer \
  --bootstrap-server kafka:9092 \
  --topic topic_clickstream \
  --from-beginning \
  --max-messages 20

# Watch external data (batch sources)
docker exec kafka kafka-console-consumer \
  --bootstrap-server kafka:9092 \
  --topic topic_external_data \
  --from-beginning \
  --max-messages 20
```

---

## ğŸ”Œ API Endpoints

### FastAPI Gateway Documentation
Open browser â†’ `http://localhost:8000/docs`

### Quick API Tests

**Health Check:**
```bash
curl http://localhost:8000/health
```

**Send User Event (Mobile App):**
```bash
curl -X POST http://localhost:8000/events/user \
  -H "Content-Type: application/json" \
  -d '{
    "event_type": "page_view",
    "user_id": "user_123",
    "session_id": "sess_abc",
    "app_type": "mobile",
    "timestamp": "2026-02-16T10:30:00Z",
    "properties": {
      "page": "booking_detail",
      "destination": "Ha Noi"
    }
  }'
```

**Send Booking Event:**
```bash
curl -X POST http://localhost:8000/events/booking \
  -H "Content-Type: application/json" \
  -d '{
    "event_type": "booking_created",
    "booking_id": "book_123",
    "user_id": "user_123",
    "properties": {
      "destination": "Da Nang",
      "price": 250.00
    }
  }'
```

**Batch Publish Events:**
```bash
curl -X POST http://localhost:8000/events/batch \
  -H "Content-Type: application/json" \
  -d '[
    {
      "event_type": "click",
      "source_app": "mobile_app",
      "data": {"button": "book_now"}
    },
    {
      "event_type": "search",
      "source_app": "web_app",
      "data": {"destination": "Ho Chi Minh"}
    }
  ]'
```

---

## ğŸ“‹ Airflow Workflows

### Access Airflow Web UI
Open browser â†’ `http://localhost:8888`
- **Username:** `airflow`
- **Password:** `airflow`

### Available DAGs

#### 1. Weather Data Ingestion
- **Schedule:** Every 6 hours (0 */6 * * *)
- **Tasks:**
  1. `fetch_weather_data` - Call weather API
  2. `transform_weather_data` - Format and enrich
  3. `publish_to_kafka` - Publish to topic_external_data
- **Output:** Weather data with temperature, humidity, wind
- **Cities:** Ha Noi, Ho Chi Minh, Da Nang, Can Tho, Hai Phong

#### 2. Maps Data Ingestion
- **Schedule:** Daily at midnight (0 0 * * *)
- **Tasks:**
  1. `fetch_maps_data` - Get location data
  2. `enrich_with_metadata` - Add analytics metrics
  3. `publish_to_kafka` - Publish enriched data
- **Output:** Maps data with coordinates, hotels, ratings

#### 3. Configuration-Driven Pipeline
- **Schedule:** Hourly (@hourly)
- **Tasks:**
  - `fetch_*` - Parallel fetch from all sources
  - `process_and_validate` - Data quality checks
  - `publish_to_kafka` - Publish validated data
- **Sources:**
  - Social Media (Twitter, Instagram, TikTok)
  - Market Data (Stocks)
  - News (RSS feeds)
- **Enable/Disable:** Modify source config in DAG

---

## ğŸ“Š Spark Streaming Analysis

### Access Spark UI
Open browser â†’ `http://localhost:8181`

### Running Streaming Jobs
```bash
# Check logs
docker logs streaming-jobs -f

# Spark is processing:
# 1. Application events (5-minute windows)
# 2. Clickstream data (10-minute windows)
# 3. CDC changes (operational metrics)
```

### View Output
```bash
# Processed events
docker exec kafka kafka-console-consumer \
  --bootstrap-server kafka:9092 \
  --topic topic_processed_events \
  --max-messages 10
```

---

## ğŸ—‚ï¸ Directory Structure

```
simulations/
â”œâ”€â”€ README.md                           # Main documentation
â”œâ”€â”€ QUICK_START.md                      # This file
â”œâ”€â”€ docker-compose-production.yml       # All services configuration
â”‚
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ api/                            # FastAPI Application Layer
â”‚   â”‚   â”œâ”€â”€ main.py                     # Gateway implementation
â”‚   â”‚   â”œâ”€â”€ Dockerfile                  # Container image
â”‚   â”‚   â””â”€â”€ requirements.txt            # Dependencies
â”‚   â”œâ”€â”€ mobile/                         # Mobile App Simulator
â”‚   â”‚   â”œâ”€â”€ simulator.py                # Event generator
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â””â”€â”€ web/                            # Web App Simulator
â”‚       â”œâ”€â”€ simulator.py                # Event generator
â”‚       â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ kafka/
â”‚   â””â”€â”€ config.py                       # Topic configurations & schemas
â”‚
â”œâ”€â”€ schema_registry/
â”‚   â”œâ”€â”€ AppEvent.avsc                   # App event schema
â”‚   â”œâ”€â”€ CDCChange.avsc                  # CDC event schema
â”‚   â”œâ”€â”€ Clickstream.avsc                # Clickstream schema
â”‚   â””â”€â”€ AppLog.avsc                     # Log schema
â”‚
â”œâ”€â”€ postgres_cdc/
â”‚   â”œâ”€â”€ simulator.py                    # CDC event generator
â”‚   â”œâ”€â”€ init.sql                        # Database initialization
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ spark_streaming/
â”‚   â”œâ”€â”€ streaming_jobs.py               # Real-time processing
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/                           # Scheduled pipelines
â”‚   â”‚   â”œâ”€â”€ weather_data_ingestion.py   # Weather API
â”‚   â”‚   â”œâ”€â”€ maps_data_ingestion.py      # Maps API
â”‚   â”‚   â””â”€â”€ config_driven_pipeline.py   # Multi-source
â”‚   â”œâ”€â”€ .env                            # Airflow config
â”‚   â”œâ”€â”€ logs/                           # Task logs
â”‚   â””â”€â”€ plugins/                        # Custom operators
â”‚
â”œâ”€â”€ clickstream/
â”‚   â””â”€â”€ simulator.py                    # Click event generator
â”‚
â””â”€â”€ external_data/
    â””â”€â”€ simulator.py                    # Multi-source simulator
```

---

## ğŸ” Monitoring Checklist

### Health Checks
```bash
# All containers running?
docker-compose ps | grep -c "Up"  # Should be 16

# Kafka healthy?
docker exec kafka kafka-broker-api-versions \
  --bootstrap-server kafka:9092

# Topics created?
docker exec kafka kafka-topics \
  --list --bootstrap-server kafka:9092

# API responding?
curl -s http://localhost:8000/health | jq

# Airflow ready?
curl -s http://localhost:8888 | head -20
```

### Data Flow Verification
1. **Application Events:**
   - âœ… Mobile simulator running: `docker logs mobile-simulator`
   - âœ… Web simulator running: `docker logs web-simulator`
   - âœ… Events in topic: Check Kafka UI / topic_app_events

2. **CDC Changes:**
   - âœ… PostgreSQL running: `docker exec postgres pg_isready`
   - âœ… CDC simulator running: `docker logs cdc-simulator`
   - âœ… Events in topic: Check Kafka UI / topic_cdc_changes

3. **Clickstream:**
   - âœ… Simulator running: `docker logs clickstream-simulator`
   - âœ… Events in topic: Check Kafka UI / topic_clickstream

4. **External Data:**
   - âœ… Airflow DAGs running: Check Airflow UI
   - âœ… Simulator running: `docker logs external-data-simulator`
   - âœ… Events in topic: Check Kafka UI / topic_external_data

5. **Streaming Processing:**
   - âœ… Spark jobs active: Check Spark UI
   - âœ… Output topics populated: Check Kafka UI / topic_processed_events

---

## âš™ï¸ Configuration Options

### Adjust Data Volume

**Mobile App Events (apps/mobile/simulator.py):**
```python
simulate_continuous_traffic(duration_minutes=30)  # Default: 30 minutes
```

**Web App Events (apps/web/simulator.py):**
```python
simulate_continuous_traffic(duration_minutes=30)  # Default: 30 minutes
```

**Clickstream Rate (clickstream/simulator.py):**
```python
events_per_minute=60  # Increase for higher volume
```

**CDC Update Frequency (postgres_cdc/simulator.py):**
- Modify `time.sleep(random.uniform(2, 5))` for frequency

**External Data Batch Size (external_data/simulator.py):**
```python
events_per_batch=50  # Adjust batch size
```

### Change Kafka Topic Partitions
Edit `docker-compose-production.yml`:
```yaml
kafka:
  environment:
    KAFKA_NUM_PARTITIONS: 12  # Default: 6
    KAFKA_DEFAULT_REPLICATION_FACTOR: 3
```

### Modify Airflow Schedule
Edit `airflow/dags/*.py`:
```python
dag = DAG(
    'weather_data',
    schedule_interval='@hourly',  # Change: '@daily', '0 */6 * * *', etc
)
```

---

## ğŸš¨ Troubleshooting

### Issue: "Topic does not exist"
```bash
# Create missing topic
docker exec kafka kafka-topics --create \
  --topic topic_app_events \
  --partitions 6 \
  --replication-factor 1 \
  --bootstrap-server kafka:9092
```

### Issue: "Connection refused"
```bash
# Wait for services to be ready
docker-compose ps

# If not healthy, restart
docker-compose restart kafka
```

### Issue: "API not accepting events"
```bash
# Check if API is running
docker logs api-gateway

# Verify health endpoint
curl http://localhost:8000/health

# Restart API
docker-compose restart api
```

### Issue: "Airflow DAGs not running"
```bash
# Check scheduler logs
docker logs airflow-scheduler -f

# Trigger DAG manually
docker exec airflow-webserver airflow dags test \
  weather_data_ingestion 2026-02-16
```

### Issue: "No events in Kafka"
```bash
# Check simulators
docker logs mobile-simulator | tail -20
docker logs cdc-simulator | tail -20
docker logs clickstream-simulator | tail -20

# Restart all simulators
docker-compose restart mobile-simulator web-simulator \
  cdc-simulator clickstream-simulator external-data-simulator
```

---

## ğŸ“ˆ Performance Tuning

### Increase Throughput
1. **Kafka:**
   - Increase partitions: `KAFKA_NUM_PARTITIONS: 12`
   - Tune batch size in producers

2. **Spark:**
   - Scale workers: Add more `spark-worker` services
   - Increase executor memory: Edit docker-compose

3. **Simulators:**
   - Reduce event delays
   - Increase concurrent users/sessions

### Reduce Latency
1. **Kafka batching:**
   ```python
   linger_ms=1,      # Reduce wait time
   batch_size=4096   # Smaller batches
   ```

2. **Spark windows:**
   - Reduce window duration: `window("1 minute")` instead of "5 minutes"

3. **Airflow:**
   - Run DAGs more frequently
   - Reduce task dependencies

---

## ğŸ“š Next Steps

1. **Explore Data:**
   - Browse topics in Kafka UI
   - Check event schemas in Schema Registry
   - Inspect messages in detail

2. **Custom Processing:**
   - Add new DAGs in `airflow/dags/`
   - Create custom Spark jobs in `spark_streaming/`
   - Extend API with new endpoints

3. **Scale Up:**
   - Multiple Spark workers
   - More Kafka brokers
   - Distributed Airflow installation

4. **Production:**
   - Add authentication (Kafka, Airflow, APIs)
   - Configure persistent storage
   - Set up monitoring/alerting
   - Implement backup strategies

---

## ğŸ“ Support

For detailed documentation, see: `README.md`
For API examples, visit: `http://localhost:8000/docs`
For architecture details, check: `docker-compose-production.yml`

---

Happy Data Platforming! ğŸš€

*Last Updated: February 16, 2026*
